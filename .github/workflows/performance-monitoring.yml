# file: .github/workflows/performance-monitoring.yml
# version: 1.1.0
# guid: performance-monitoring-workflow-2025

name: Performance Monitoring

on:
  schedule:
    - cron: "0 3 * * *"
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "benchmarks/**"
      - "scripts/**"
      - ".github/workflows/performance-monitoring.yml"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  rust-benchmarks:
    name: Rust Benchmarks
    runs-on: ubuntu-latest
    outputs:
      average: ${{ steps.capture.outputs.average }}
      best: ${{ steps.capture.outputs.best }}
      worst: ${{ steps.capture.outputs.worst }}
    steps:
      - uses: actions/checkout@v5

      - name: Install toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Restore cache
        uses: Swatinem/rust-cache@v2

      - name: Build benchmark target
        run: cargo build --manifest-path testdata/rust/Cargo.toml --release

      - name: Prepare benchmark directories
        run: |
          mkdir -p benchmarks
          mkdir -p .github/benchmark-data

      - name: Measure Rust benchmark
        run: |
          python3 scripts/benchmarks/measure_command.py \
            --label "Rust Fixture Execution" \
            --command "./target/release/ghcommon-test-fixture" \
            --workdir testdata/rust \
            --iterations 10 \
            --output benchmarks/rust.json \
            --metadata "command: ./target/release/ghcommon-test-fixture"

      - name: Capture benchmark metrics
        id: capture
        run: |
          python3 - <<'PY'
          import json
          import math
          import os

          with open("benchmarks/rust.json", "r", encoding="utf-8") as handle:
              entry = json.load(handle)[0]

          def fmt(value: float | None) -> str:
              if value is None or (isinstance(value, float) and math.isnan(value)):
                  return "n/a"
              return f"{value:.6f}"

          average = entry.get("average_seconds", entry.get("value"))
          best = entry.get("best_seconds", entry.get("value"))
          worst = entry.get("worst_seconds", entry.get("value"))

          output = os.environ["GITHUB_OUTPUT"]
          with open(output, "a", encoding="utf-8") as fh:
              fh.write(f"average={fmt(average)}\n")
              fh.write(f"best={fmt(best)}\n")
              fh.write(f"worst={fmt(worst)}\n")
          PY

      - name: Publish benchmark result
        uses: jdfalk/github-action-benchmark@main
        with:
          name: Rust Fixture Execution
          tool: "customSmallerIsBetter"
          output-file-path: benchmarks/rust.json
          summary-always: true
          github-token: ${{ secrets.BENCHMARK_ACTION_TOKEN || secrets.GITHUB_TOKEN }}
          external-data-json-path: .github/benchmark-data/rust.json

      - name: Upload rust benchmark artifacts
        uses: actions/upload-artifact@v5
        with:
          name: rust-benchmarks
          path: |
            benchmarks/rust.json
            .github/benchmark-data/rust.json

  node-benchmarks:
    name: Node.js Benchmarks
    runs-on: ubuntu-latest
    outputs:
      average: ${{ steps.capture.outputs.average }}
      best: ${{ steps.capture.outputs.best }}
      worst: ${{ steps.capture.outputs.worst }}
    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-node@v6
        with:
          node-version: 20

      - name: Install dependencies
        working-directory: testdata/node
        run: npm install

      - name: Prepare benchmark directories
        run: |
          mkdir -p benchmarks
          mkdir -p .github/benchmark-data

      - name: Measure Node.js benchmark
        run: |
          python3 scripts/benchmarks/measure_command.py \
            --label "Node Fixture Execution" \
            --command "node index.js" \
            --workdir testdata/node \
            --iterations 10 \
            --output benchmarks/node.json \
            --metadata "command: node index.js"

      - name: Capture benchmark metrics
        id: capture
        run: |
          python3 - <<'PY'
          import json
          import math
          import os

          with open("benchmarks/node.json", "r", encoding="utf-8") as handle:
              entry = json.load(handle)[0]

          def fmt(value: float | None) -> str:
              if value is None or (isinstance(value, float) and math.isnan(value)):
                  return "n/a"
              return f"{value:.6f}"

          average = entry.get("average_seconds", entry.get("value"))
          best = entry.get("best_seconds", entry.get("value"))
          worst = entry.get("worst_seconds", entry.get("value"))

          output = os.environ["GITHUB_OUTPUT"]
          with open(output, "a", encoding="utf-8") as fh:
              fh.write(f"average={fmt(average)}\n")
              fh.write(f"best={fmt(best)}\n")
              fh.write(f"worst={fmt(worst)}\n")
          PY

      - name: Publish benchmark result
        uses: jdfalk/github-action-benchmark@main
        with:
          name: Node Fixture Execution
          tool: "customSmallerIsBetter"
          output-file-path: benchmarks/node.json
          summary-always: true
          github-token: ${{ secrets.BENCHMARK_ACTION_TOKEN || secrets.GITHUB_TOKEN }}
          external-data-json-path: .github/benchmark-data/node.json

      - name: Upload node benchmark artifacts
        uses: actions/upload-artifact@v5
        with:
          name: node-benchmarks
          path: |
            benchmarks/node.json
            .github/benchmark-data/node.json

  python-benchmarks:
    name: Python Benchmarks
    runs-on: ubuntu-latest
    outputs:
      average: ${{ steps.capture.outputs.average }}
      best: ${{ steps.capture.outputs.best }}
      worst: ${{ steps.capture.outputs.worst }}
    steps:
      - uses: actions/checkout@v5

      - uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Prepare benchmark directories
        run: |
          mkdir -p benchmarks
          mkdir -p .github/benchmark-data

      - name: Measure Python benchmark
        run: |
          python3 scripts/benchmarks/measure_command.py \
            --label "Python Fixture Execution" \
            --command "python hello_world.py" \
            --workdir testdata/python \
            --iterations 10 \
            --output benchmarks/python.json \
            --metadata "command: python hello_world.py"

      - name: Capture benchmark metrics
        id: capture
        run: |
          python3 - <<'PY'
          import json
          import math
          import os

          with open("benchmarks/python.json", "r", encoding="utf-8") as handle:
              entry = json.load(handle)[0]

          def fmt(value: float | None) -> str:
              if value is None or (isinstance(value, float) and math.isnan(value)):
                  return "n/a"
              return f"{value:.6f}"

          average = entry.get("average_seconds", entry.get("value"))
          best = entry.get("best_seconds", entry.get("value"))
          worst = entry.get("worst_seconds", entry.get("value"))

          output = os.environ["GITHUB_OUTPUT"]
          with open(output, "a", encoding="utf-8") as fh:
              fh.write(f"average={fmt(average)}\n")
              fh.write(f"best={fmt(best)}\n")
              fh.write(f"worst={fmt(worst)}\n")
          PY

      - name: Publish benchmark result
        uses: jdfalk/github-action-benchmark@main
        with:
          name: Python Fixture Execution
          tool: "customSmallerIsBetter"
          output-file-path: benchmarks/python.json
          summary-always: true
          github-token: ${{ secrets.BENCHMARK_ACTION_TOKEN || secrets.GITHUB_TOKEN }}
          external-data-json-path: .github/benchmark-data/python.json

      - name: Upload python benchmark artifacts
        uses: actions/upload-artifact@v5
        with:
          name: python-benchmarks
          path: |
            benchmarks/python.json
            .github/benchmark-data/python.json
          if-no-files-found: ignore

  benchmark-summary:
    name: Benchmark Summary
    needs: [rust-benchmarks, node-benchmarks, python-benchmarks]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Summarize results
        env:
          RUST_AVG: ${{ needs.rust-benchmarks.outputs.average }}
          RUST_BEST: ${{ needs.rust-benchmarks.outputs.best }}
          RUST_WORST: ${{ needs.rust-benchmarks.outputs.worst }}
          NODE_AVG: ${{ needs.node-benchmarks.outputs.average }}
          NODE_BEST: ${{ needs.node-benchmarks.outputs.best }}
          NODE_WORST: ${{ needs.node-benchmarks.outputs.worst }}
          PYTHON_AVG: ${{ needs.python-benchmarks.outputs.average }}
          PYTHON_BEST: ${{ needs.python-benchmarks.outputs.best }}
          PYTHON_WORST: ${{ needs.python-benchmarks.outputs.worst }}
        run: |
          {
            printf '## Benchmark Summary\n\n'
            printf '| Suite | Average (s) | Best (s) | Worst (s) |\n'
            printf '|-------|-------------|----------|-----------|\n'
            printf '| Rust | %s | %s | %s |\n' "${RUST_AVG:-n/a}" "${RUST_BEST:-n/a}" "${RUST_WORST:-n/a}"
            printf '| Node.js | %s | %s | %s |\n' "${NODE_AVG:-n/a}" "${NODE_BEST:-n/a}" "${NODE_WORST:-n/a}"
            printf '| Python | %s | %s | %s |\n' "${PYTHON_AVG:-n/a}" "${PYTHON_BEST:-n/a}" "${PYTHON_WORST:-n/a}"
            printf '\n_View individual suite logs for regression alerts and charts._\n'
          } >> "$GITHUB_STEP_SUMMARY"
